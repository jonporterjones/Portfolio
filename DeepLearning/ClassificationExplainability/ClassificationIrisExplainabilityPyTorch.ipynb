{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook will explore explainability as it relates to Deep Learning models; particuarly clasification tasks.\n",
    "\n",
    "### Update\n",
    "\n",
    "This is currently a work-in progress, but is looking promising.\n",
    "It can identify active neurons and identifying different neurons used for different output classes.\n",
    "It can look at the weights for each of these neurons and see what input features are relevant for what active neurons.\n",
    "\n",
    "## Justification\n",
    "\n",
    "A neuron in a neural network is composed of a linear function followed by a non-linear activation function.  The purpose of this notebook is to explore input features/values lead to specific classifications.  I've never had great intuition on how different neurons support classification in a multi-class setting, so I wanted to use this notebook to explore this a little further and improve upon my intuition.\n",
    "\n",
    "## Approach\n",
    "\n",
    "The way to do this is to inspect the neurons in the neaural network; look at the feature weights and activated nodes for different output classes.  The intuition is that certain nodes would learn to identify features that identify a specific output and these features would be activated for specific output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "Use the IRIS dataset, create a dataset and dataloader and a simple train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data,\n",
    "                                                    iris.target,\n",
    "                                                    test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the train and test data is essential and helps the model converge faster and find a lower mimimum for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize x values\n",
    "# Compute mean and standard deviation\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "\n",
    "# Perform standardization\n",
    "# When standardizing the test set it is important to use the training set mean/std  \n",
    "#  otherwise information about your test data will bleed into your evaluation.\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a custom Dataset class, implement the interface provided by torch.utils.data.Dataset.  This includes functions __init__, __len__, __getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(data=x,\n",
    "                              dtype=torch.float32)\n",
    "        self.y = torch.tensor(data=y,\n",
    "                              dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[index], self.y[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create instances of the IrisDataset class.  \n",
    "One for training data and one for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = IrisDataset(x=x_train, y=y_train)\n",
    "test_data = IrisDataset(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 135\n",
      "Test: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {len(train_data)}')\n",
    "print(f'Test: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader class is used to build training batches and optionally shuffle the data at each epoch.  \n",
    "Use a large batch size for this trivial problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The neural network model  class defined as a subclass of torch.nn.Module.  \n",
    "The __init__ function is used to define the network, input and output shapes, hidden layers, and the final layer.\n",
    "The node of a hidden layer in a neural network is a linear function, followed by a non-linear activation function.  The network definition below includes each linear function and each activation function.\n",
    "\n",
    "The code below will create a simple network with 1 hidden layer.  The inputs initially have 4 dimensions, the hidden layer will expand this to 12 nodes dimensions, the output layer will collapse down to 3 dimensions (the number of output classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified to return activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(4, 12)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.output = nn.Linear(12, 3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidden1 = self.hidden1(x)\n",
    "        activation1 = self.activation1(hidden1)\n",
    "        output = self.output(activation1)\n",
    "\n",
    "        # Log the explainability aspects if in training mode.\n",
    "        #if not self.training:\n",
    "        #    print(hidden1.weight)\n",
    "        #    print(hidden1.bias)\n",
    "        #     print(f'hidden1: {hidden1.shape}')\n",
    "        #     print(hidden1)\n",
    "        #     print(f'activation1: {activation1.shape}')\n",
    "        #     print(activation1)\n",
    "        #     print(f'output: {output.shape}')\n",
    "        #     print(output)\n",
    "            \n",
    "        return (output, activation1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of IrisNetwork and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisNetwork(\n",
      "  (hidden1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (activation1): ReLU()\n",
      "  (output): Linear(in_features=12, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = IrisNetwork().to(\"cpu\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "A neural network needs a loss function and an optimizer in order to be trained.\n",
    "\n",
    "The IRIS dataset will use CrossEntropyLoss which expects logits output equal to the number of output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.5e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Training requires iterations over the batches of data made available via the DataLoader.  \n",
    "Each pass over the entire dataset is considered an epoch, the training loop will usually be run for multiple epochs.  \n",
    "   \n",
    "The example below trains for a large number of epochs (150).  This is more than enough to build a perfect classifier for this trivial problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train() # Puts model into train mode.\n",
    "    epoch_loss = 0.0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()   # 0 out gradents for next computation.\n",
    "\n",
    "        # forward propagation\n",
    "        pred = model(X)         # Get predicted values for inputs\n",
    "        loss = loss_fn(pred[0], y) # Compute loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()         # Compute gradients\n",
    "        optimizer.step()        # Update trainable parameters.\n",
    "\n",
    "        # print/track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.04132878780365\n",
      "Epoch 2 loss: 0.6209415793418884\n",
      "Epoch 3 loss: 0.4804547429084778\n",
      "Epoch 4 loss: 0.40877512097358704\n",
      "Epoch 5 loss: 0.352464884519577\n",
      "Epoch 6 loss: 0.3091789186000824\n",
      "Epoch 7 loss: 0.28199881315231323\n",
      "Epoch 8 loss: 0.26609018445014954\n",
      "Epoch 9 loss: 0.24823133647441864\n",
      "Epoch 10 loss: 0.22484511137008667\n",
      "Epoch 11 loss: 0.19808275997638702\n",
      "Epoch 12 loss: 0.17278091609477997\n",
      "Epoch 13 loss: 0.15202809870243073\n",
      "Epoch 14 loss: 0.13115404546260834\n",
      "Epoch 15 loss: 0.11192416399717331\n",
      "Epoch 16 loss: 0.09525609761476517\n",
      "Epoch 17 loss: 0.0824543684720993\n",
      "Epoch 18 loss: 0.07284078747034073\n",
      "Epoch 19 loss: 0.06442566961050034\n",
      "Epoch 20 loss: 0.05775027349591255\n",
      "Epoch 21 loss: 0.051503829658031464\n",
      "Epoch 22 loss: 0.04912145063281059\n",
      "Epoch 23 loss: 0.046925172209739685\n",
      "Epoch 24 loss: 0.04560451582074165\n",
      "Epoch 25 loss: 0.04464007541537285\n",
      "Epoch 26 loss: 0.04315115138888359\n",
      "Epoch 27 loss: 0.0414748340845108\n",
      "Epoch 28 loss: 0.04051163047552109\n",
      "Epoch 29 loss: 0.040024664252996445\n",
      "Epoch 30 loss: 0.03959420323371887\n",
      "Epoch 31 loss: 0.03915935754776001\n",
      "Epoch 32 loss: 0.03868798539042473\n",
      "Epoch 33 loss: 0.03838665038347244\n",
      "Epoch 34 loss: 0.038322482258081436\n",
      "Epoch 35 loss: 0.0385177843272686\n",
      "Epoch 36 loss: 0.038834575563669205\n",
      "Epoch 37 loss: 0.039067283272743225\n",
      "Epoch 38 loss: 0.039069417864084244\n",
      "Epoch 39 loss: 0.038895219564437866\n",
      "Epoch 40 loss: 0.038676872849464417\n",
      "Epoch 41 loss: 0.03853912279009819\n",
      "Epoch 42 loss: 0.03846680000424385\n",
      "Epoch 43 loss: 0.03842602297663689\n",
      "Epoch 44 loss: 0.03835902363061905\n",
      "Epoch 45 loss: 0.03828743100166321\n",
      "Epoch 46 loss: 0.03822539374232292\n",
      "Epoch 47 loss: 0.03821572661399841\n",
      "Epoch 48 loss: 0.03824325278401375\n",
      "Epoch 49 loss: 0.0382881835103035\n",
      "Epoch 50 loss: 0.03829765319824219\n",
      "Epoch 51 loss: 0.03826134651899338\n",
      "Epoch 52 loss: 0.03818318620324135\n",
      "Epoch 53 loss: 0.038100022822618484\n",
      "Epoch 54 loss: 0.0380234494805336\n",
      "Epoch 55 loss: 0.037960126996040344\n",
      "Epoch 56 loss: 0.03789196163415909\n",
      "Epoch 57 loss: 0.037819817662239075\n",
      "Epoch 58 loss: 0.0377422459423542\n",
      "Epoch 59 loss: 0.037678372114896774\n",
      "Epoch 60 loss: 0.037632208317518234\n",
      "Epoch 61 loss: 0.03760794922709465\n",
      "Epoch 62 loss: 0.03758933022618294\n",
      "Epoch 63 loss: 0.03756719455122948\n",
      "Epoch 64 loss: 0.03753611072897911\n",
      "Epoch 65 loss: 0.03750211372971535\n",
      "Epoch 66 loss: 0.03747363016009331\n",
      "Epoch 67 loss: 0.03745044395327568\n",
      "Epoch 68 loss: 0.037431564182043076\n",
      "Epoch 69 loss: 0.03741070255637169\n",
      "Epoch 70 loss: 0.03738962486386299\n",
      "Epoch 71 loss: 0.037371259182691574\n",
      "Epoch 72 loss: 0.03736027702689171\n",
      "Epoch 73 loss: 0.0373576357960701\n",
      "Epoch 74 loss: 0.037358663976192474\n",
      "Epoch 75 loss: 0.03736009821295738\n",
      "Epoch 76 loss: 0.03735887631773949\n",
      "Epoch 77 loss: 0.03735671192407608\n",
      "Epoch 78 loss: 0.03735501319169998\n",
      "Epoch 79 loss: 0.03735358268022537\n",
      "Epoch 80 loss: 0.03735143318772316\n",
      "Epoch 81 loss: 0.037346646189689636\n",
      "Epoch 82 loss: 0.03734029456973076\n",
      "Epoch 83 loss: 0.03733401000499725\n",
      "Epoch 84 loss: 0.03732898831367493\n",
      "Epoch 85 loss: 0.03732543811202049\n",
      "Epoch 86 loss: 0.037321776151657104\n",
      "Epoch 87 loss: 0.03731725364923477\n",
      "Epoch 88 loss: 0.03731222078204155\n",
      "Epoch 89 loss: 0.03730776533484459\n",
      "Epoch 90 loss: 0.03730391710996628\n",
      "Epoch 91 loss: 0.037299685180187225\n",
      "Epoch 92 loss: 0.037294529378414154\n",
      "Epoch 93 loss: 0.03728882968425751\n",
      "Epoch 94 loss: 0.037282977253198624\n",
      "Epoch 95 loss: 0.03727759048342705\n",
      "Epoch 96 loss: 0.037272825837135315\n",
      "Epoch 97 loss: 0.037268273532390594\n",
      "Epoch 98 loss: 0.037264250218868256\n",
      "Epoch 99 loss: 0.03726048395037651\n",
      "Epoch 100 loss: 0.037257108837366104\n",
      "Epoch 101 loss: 0.03725428879261017\n",
      "Epoch 102 loss: 0.037251636385917664\n",
      "Epoch 103 loss: 0.03724871575832367\n",
      "Epoch 104 loss: 0.03724581375718117\n",
      "Epoch 105 loss: 0.0372430756688118\n",
      "Epoch 106 loss: 0.037240296602249146\n",
      "Epoch 107 loss: 0.03723776340484619\n",
      "Epoch 108 loss: 0.03723514825105667\n",
      "Epoch 109 loss: 0.03723243251442909\n",
      "Epoch 110 loss: 0.03722980618476868\n",
      "Epoch 111 loss: 0.03722739219665527\n",
      "Epoch 112 loss: 0.03722531348466873\n",
      "Epoch 113 loss: 0.037223123013973236\n",
      "Epoch 114 loss: 0.03722082078456879\n",
      "Epoch 115 loss: 0.03721854090690613\n",
      "Epoch 116 loss: 0.03721645101904869\n",
      "Epoch 117 loss: 0.03721423074603081\n",
      "Epoch 118 loss: 0.03721201419830322\n",
      "Epoch 119 loss: 0.03720983490347862\n",
      "Epoch 120 loss: 0.037207771092653275\n",
      "Epoch 121 loss: 0.03720567747950554\n",
      "Epoch 122 loss: 0.037203598767519\n",
      "Epoch 123 loss: 0.03720146790146828\n",
      "Epoch 124 loss: 0.037199463695287704\n",
      "Epoch 125 loss: 0.03719785064458847\n"
     ]
    }
   ],
   "source": [
    "epochs = 125\n",
    "for t in range(epochs):\n",
    "    epoch_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(f'Epoch {t+1} loss: {epoch_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "The test dataset does not require a loop or the mini-batches,\n",
    "Simply compute the test predictions in one call to the trained model.\n",
    "test_predictions is wrapped in a softmax to ensure that the predictions are probabilities that sum to 1.  This is common in classification tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (MSE) on test dataset 0.09417455643415451\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Put model in evaluation mode.\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(test_data.x)[0]\n",
    "    test_loss = loss_fn(test_predictions, test_data.y).item()\n",
    "    test_predictions = torch.softmax(test_predictions, dim=1)\n",
    "\n",
    "print(f'Loss (MSE) on test dataset {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1890, -1.3447,  3.0912, -1.0852,  0.4250,  0.4312, -0.3905, -1.4740,\n",
       "        -0.9926, -0.5294, -1.8060,  0.7147], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(model.hidden1.weight[0] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.7588,  0.6556,  2.2778,  0.5642],\n",
       "        [-0.5034,  0.4696, -0.9210, -0.8904],\n",
       "        [ 1.4754, -0.2611, -0.5847, -1.3288],\n",
       "        [-1.2014,  1.1262, -0.8873, -0.7997],\n",
       "        [-1.2014,  1.4520, -1.4586, -1.6997],\n",
       "        [-0.6982,  1.2390, -1.5964, -1.3437],\n",
       "        [-1.4773,  1.0019, -1.7186, -1.9799],\n",
       "        [-0.3087, -0.6222,  2.8651,  1.9852],\n",
       "        [-0.5809,  1.2950, -0.8598, -0.4670],\n",
       "        [-1.1990,  1.1826, -1.4767, -1.4573],\n",
       "        [-0.9695, -1.3006,  2.2729,  2.6452],\n",
       "        [-0.8673,  1.2865, -1.6217, -1.7191]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute precision, recall, and f1 score for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set precision score: [1.   1.   0.75]\n",
      "test set recall score: [1.         0.88888889 1.        ]\n",
      "test set f1 score: [1.         0.94117647 0.85714286]\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(test_data.y.numpy(), torch.argmax(test_predictions, dim=1), average=None)\n",
    "print(f\"test set precision score: {precision}\")\n",
    "\n",
    "recall = recall_score(test_data.y.numpy(), torch.argmax(test_predictions, dim=1), average=None)\n",
    "print(f\"test set recall score: {recall}\")\n",
    "\n",
    "f1 = f1_score(test_data.y.numpy(), torch.argmax(test_predictions, dim=1), average=None)\n",
    "print(f\"test set f1 score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions_and_explainability(model, test_x, test_y):\n",
    "    \n",
    "    activations_by_output_class = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: []\n",
    "    }\n",
    "\n",
    "    model.eval() # Put model in evaluation mode.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions, activations = model(test_x)\n",
    "\n",
    "    # modify the predictions to get the softmax and the argmax\n",
    "    predictions = torch.argmax(torch.softmax(predictions, dim=1), dim=1)\n",
    "\n",
    "    for prediction, truth, activation in zip(predictions, test_y, activations):\n",
    "        print(prediction)\n",
    "        print(truth)\n",
    "        print(activation)\n",
    "        print('')\n",
    "\n",
    "        activation_on_indicies = np.where(activation > 0)\n",
    "        for hidden_neuron in activation_on_indicies[0]:\n",
    "            relevant_input_features = np.where(model.hidden1.weight[hidden_neuron] > 0)[0]\n",
    "\n",
    "        if prediction == 0:\n",
    "            activations_by_output_class[0].append((activation_on_indicies, relevant_input_features))\n",
    "        elif prediction == 1:\n",
    "            activations_by_output_class[1].append((activation_on_indicies, relevant_input_features))\n",
    "        else:\n",
    "            activations_by_output_class[2].append((activation_on_indicies, relevant_input_features))\n",
    "\n",
    "    #for predicted_class, activation_list in activations_by_output_class.items():\n",
    "    #    for activation in activation_list:\n",
    "    #        relevant_input_features = np.where(model.hidden1.weight[0] > 0)\n",
    "\n",
    "    print(activations_by_output_class)\n",
    "\n",
    "    # The weights and bias are accesible for each layer,\n",
    "    # add some intuition about which input features matter for which active neurons.\n",
    "    # Weights of hidden layer 1 are shape 12x4 (output_size, input_size))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([2.3114, 0.0000, 4.0551, 0.0000, 0.0000, 0.0000, 0.0000, 0.1194, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000])\n",
      "\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor([0.0000, 2.5618, 3.9603, 4.7513, 9.0245, 7.8111, 8.0532, 0.0000, 4.2812,\n",
      "        7.1017, 0.0000, 8.8966])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([0.0000, 0.0000, 2.5706, 0.0000, 0.0000, 0.0000, 0.0000, 0.1333, 0.0000,\n",
      "        0.0000, 0.4885, 0.0000])\n",
      "\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor([4.5970, 0.0000, 2.6815, 0.0000, 0.0000, 0.0000, 0.0000, 3.3796, 0.0000,\n",
      "        0.0000, 2.5830, 0.0000])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([1.4763, 0.0000, 2.9840, 0.0000, 0.0000, 0.0000, 0.0000, 0.7156, 0.0000,\n",
      "        0.0000, 0.2888, 0.0000])\n",
      "\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor([0.0000, 1.3395, 4.2172, 2.3369, 5.7351, 5.1224, 5.3196, 0.0000, 1.8677,\n",
      "        4.3426, 0.0000, 5.9462])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([2.2824, 0.0000, 3.8887, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([0.3658, 0.0000, 3.4428, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000])\n",
      "\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor([3.7586, 0.0000, 2.6738, 0.0000, 0.0000, 0.0000, 0.0000, 2.7222, 0.0000,\n",
      "        0.0000, 1.9041, 0.0000])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([0.0000, 0.0000, 4.1445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.2626, 0.0000])\n",
      "\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor([0.0000, 1.9536, 4.2044, 3.4654, 7.3212, 6.4099, 6.6979, 0.0000, 2.9547,\n",
      "        5.6792, 0.0000, 7.3832])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([0.0000, 0.0000, 2.3424, 0.0000, 0.3119, 0.0331, 0.6233, 0.0000, 0.0000,\n",
      "        0.0000, 0.0309, 0.5312])\n",
      "\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor([2.0110, 0.0000, 1.7626, 0.0000, 0.0000, 0.0000, 0.0000, 2.3030, 0.0000,\n",
      "        0.0000, 2.0828, 0.0000])\n",
      "\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor([1.9344, 0.0000, 1.7394, 0.0000, 0.0000, 0.0000, 0.0000, 1.5375, 0.0000,\n",
      "        0.0000, 1.1023, 0.0000])\n",
      "\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor([0.4130, 0.0000, 1.9500, 0.0000, 0.0000, 0.0000, 0.0000, 0.6902, 0.0000,\n",
      "        0.0000, 0.6458, 0.0000])\n",
      "\n",
      "{0: [((array([ 1,  2,  3,  4,  5,  6,  8,  9, 11]),), array([1])), ((array([ 1,  2,  3,  4,  5,  6,  8,  9, 11]),), array([1])), ((array([ 1,  2,  3,  4,  5,  6,  8,  9, 11]),), array([1]))], 1: [((array([0, 2, 7]),), array([2, 3])), ((array([ 2,  7, 10]),), array([2, 3])), ((array([ 0,  2,  7, 10]),), array([2, 3])), ((array([0, 2]),), array([0])), ((array([0, 2]),), array([0])), ((array([ 2, 10]),), array([2, 3])), ((array([ 2,  4,  5,  6, 10, 11]),), array([1])), ((array([ 0,  2,  7, 10]),), array([2, 3]))], 2: [((array([ 0,  2,  7, 10]),), array([2, 3])), ((array([ 0,  2,  7, 10]),), array([2, 3])), ((array([ 0,  2,  7, 10]),), array([2, 3])), ((array([ 0,  2,  7, 10]),), array([2, 3]))]}\n"
     ]
    }
   ],
   "source": [
    "test_predictions_and_explainability(model, test_data.x, test_data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The purpose of this notebook is to use a network trained on a classification task and see if we can look into what neurons are active and what features are being used to activate these neurons.\n",
    "\n",
    "This is certainly possible and this notebook is a start in that direction.  With a lot more polish, one could imagine a visual representation of a neural network showing the path that a given input would follow to arrive at a certain output.  This could go a long way towards making the neural network black box more explainable and intuitive.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
